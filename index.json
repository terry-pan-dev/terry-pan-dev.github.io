[{"authors":["admin"],"categories":null,"content":"Taotao Pan is a student of Data Science at the University of Queensland. His research interests include Machine Learning, Data Science and Software Engineering. He tends to solve real world problems by thinking like a engineer.\n","date":1564185600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1564185600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://terry-pan-dev.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Taotao Pan is a student of Data Science at the University of Queensland. His research interests include Machine Learning, Data Science and Software Engineering. He tends to solve real world problems by thinking like a engineer.","tags":null,"title":"Terry Pan","type":"authors"},{"authors":[],"categories":null,"content":" Slides can be added in a few ways:\nCreate slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://terry-pan-dev.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"ef39deeb21e92494f4940611ebedc202","permalink":"https://terry-pan-dev.github.io/project/blogguru/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/project/blogguru/","section":"project","summary":"a frond-end project for blog system","tags":["python"],"title":"blgguru","type":"project"},{"authors":null,"categories":["distributed system"],"content":" Distributed file system Introduction Both GFS and HDFS are distributed file system. GFS stands for Google file system, it’s designed by google while HDFS is an open source version of distributed file system that referenced GFS. The motivation that we need distributed file system was the astronomical increase on data. Google has designed GFS because the huge increase of index of their search engine. The creator of HDFS is also the creator of Lucene search enginee has encounter similar problems. However, to design a distributed system is not as simple as design a single machine file system. There are several challenges to be overcome.\nfault tolerance high performance network communication replicas consistency The archetecture of GFS is designed as master server and chunkserver, master server is reponse for manage the namespace, access control, and mapping from the files to chunks. Whereas chunk server is response storing the real data object.\ngfs\nlet’s explain the process of creating a file in GFS, first of all, client sends information about the file or object it wants to store on GFS by the file name chunk index to the master server of GFS then master server will start scheduling which chunk server to store file, the location of the chunk, after finishing the negotiation the master server will send back the necessary information to client typically the chunk handle and chunk location. Afterwards, client can talk to chunk server directly.\nmaster server failure -\u0026gt; operation logs chunk server failure -\u0026gt; heartbeat signal\nwrite: 1. master find the most up to date chunk server (check the version number) 2. pick one as primary rest as secondary server 3. increment version number 4. master server tells client who is primary and secondary chunk server 5. when all secondary server say yes to primary, primary say yes to client, otherwise, say no\nsuccess in many google applicatioins that relies on the underline distributed system, however, there are still several bottlenecks, like only one master that has to handle thousands of requests or master server need huge amount of memory to store the chunk server handle information.\n","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"1c339a2d33dcad60362ded6d917174dd","permalink":"https://terry-pan-dev.github.io/post/gfs-and-hdfs/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/post/gfs-and-hdfs/","section":"post","summary":"GFS and HDFS distributed file system","tags":["distributed system"],"title":"GFS and HDFS","type":"post"},{"authors":null,"categories":["distributed system"],"content":" Hadoop Introduction Hadoop is a distributed system created based on google’s GFS, the reason for the creator to create Hadoop is because when the creator creates lucene search engine he found there are huge amount of data needs to be stored into distributed system to be searched.\nHadoop is a system that consists several sub-systems, like HDFS the file system, MapReduce the computation system and yarn the resource scheduler along with additional projects like Pig, Hive, Sqoop, etc.\nDifferent queues There are several advantages of using AWS SQS service, it’s fully managed service which means you don’t have to worry about the duribility and scalability of the service, of course you don’t have to manage the server. SQS is generally used to decouple your architecture which provides more reliable system. SQS provides two types of message queue, one is standard queue, standard queue guarantees at-least-once message delivery. Therefore, when your application requires the sensitivity of delivery once message, you have to choose FIFO queue. What’s more, standard queue will do its best to keep the message deliver in order. However, there is no guarantee for the order. Generally to say, standard queue provides a simple queue service, it can be used in an application without the restriction of delivery order and times. In the contrary, FIFO queue supports strong ordering and exactly-once message delivery. Hence, if your application has these restrictions you’d better choose FIFO queue. Another important feature of SQS is that if you have several clients want to get a message, SQS is not the proper service to pick, instead using SNS.\nSQS key attributes There are several important settings worth to know when you creating a new queue. First of all, default visibility timeout is an important property. Here is an image that from the aws official documents can help me explain it well. when you push a message into the queue, the message will be copied into several servers (high availibity). Then the message is sitting there waiting for other service to pick (EC2, lambda, etc.). Whenever there is a service picking the message the visibility time out clock starts count while the message is not visible any more. Suppose the task (message) can be finished in the timeout peroid, the message will be deleted from the queue. Therefore, best practice to setup this timeout is setting it greater than the task processing time, otherwise, while the task is still processing, when the visibility timeout ends, this message will be visible by another service (process) which cause duplicated processing.\nNext two settings are simple, message retention period is the max time the queue can keep the message while no process picks it. And maximum message size is simply as the name says the max message size. Delivery delay is the time that when a message reach the queue, the message will be freezed from this amount of time, notice this time is applied for every new reached message. Next import attribute is receive message wait time, this attribute sometimes is referred as long polling. It works as following, whenever there is new process trying to pick a message, when this attribute is setting as 0, if the process finds the queue is empty, it will not wait for the new message arrives, the process will immediately shutdown the connection, in such scenaro, unnecessary connections will be established multiple times which costs extra billing. Therefore, normally we will set the value more than 0.\nDead letter queue dead letter queue is another queue you have to associate with a queue you have created. Which used to store those messages cannot be processed by that queue. For example, you can create another process which has more CPU and memory power to process the message that cannot be processed from the standard queue. What’s more, you might want to use dead letter queue to diagnose why the message cannot be processed in the standard queue. There is only one important attribute, maximum receives, this attribute is used to determine after how many processes tried the message failed, then put the message into the dead letter queue.\nAbout the encryption option, please refer my another article called AWS KMS\nOther attributes when turn on the functionality of content-based deduplication. The duplicated message will be removed automatically by the queues. Message available is the metric that shows amount of messages that you pushed into the queue and Messages in Flight is the total amount of messages that is processing.\nThere are several scenarios a SQS queue can be used, normally combine SQS queue with lambda is a good practice, you can trigger a lambda function whenever there is a new message arrives the queue. Another use case is combining SQS with SNS service to create a fan out architecture. Because, SQS can guarantee the message comes from a unique resource, whereas SNS can response the multiple delivery work. the image above shows such architecture, the leftmost is the SQS service, the middle one is SNS service, suppose you have one message that has to be processed by different services, maybe one service is doing filtering, another service is for storing this message.\n","date":1582675200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582675200,"objectID":"d69429da6aef0a033a430011ce8a21d6","permalink":"https://terry-pan-dev.github.io/post/hadoop/","publishdate":"2020-02-26T00:00:00Z","relpermalink":"/post/hadoop/","section":"post","summary":"hadoop distributed system","tags":["distributed system"],"title":"Hadoop","type":"post"},{"authors":null,"categories":["AWS"],"content":" AWS SQS Introduction AWS provides several messaging services. For example, SQS, SNS and Amazon MQ. Among these three, SNS is a pub/sub messaging system, it’s commonly used as a notification system, for example, subscribe a topic from SNS and whenever there is an update notification, SNS will push the notification to end devices (smartphones, tablets). Whereas, amazon MQ is a mature messaging solution which can integrate the on-premises messaging system with the minimum burden. In this article, I am going to give details and best practices of using AWS SQS.\nDifferent queues There are several advantages of using AWS SQS service, it’s fully managed service which means you don’t have to worry about the duribility and scalability of the service, of course you don’t have to manage the server. SQS is generally used to decouple your architecture which provides more reliable system. SQS provides two types of message queue, one is standard queue, standard queue guarantees at-least-once message delivery. Therefore, when your application requires the sensitivity of delivery once message, you have to choose FIFO queue. What’s more, standard queue will do its best to keep the message deliver in order. However, there is no guarantee for the order. Generally to say, standard queue provides a simple queue service, it can be used in an application without the restriction of delivery order and times. In the contrary, FIFO queue supports strong ordering and exactly-once message delivery. Hence, if your application has these restrictions you’d better choose FIFO queue. Another important feature of SQS is that if you have several clients want to get a message, SQS is not the proper service to pick, instead using SNS.\nSQS key attributes There are several important settings worth to know when you creating a new queue. First of all, default visibility timeout is an important property. Here is an image that from the aws official documents can help me explain it well. when you push a message into the queue, the message will be copied into several servers (high availibity). Then the message is sitting there waiting for other service to pick (EC2, lambda, etc.). Whenever there is a service picking the message the visibility time out clock starts count while the message is not visible any more. Suppose the task (message) can be finished in the timeout peroid, the message will be deleted from the queue. Therefore, best practice to setup this timeout is setting it greater than the task processing time, otherwise, while the task is still processing, when the visibility timeout ends, this message will be visible by another service (process) which cause duplicated processing.\nNext two settings are simple, message retention period is the max time the queue can keep the message while no process picks it. And maximum message size is simply as the name says the max message size. Delivery delay is the time that when a message reach the queue, the message will be freezed from this amount of time, notice this time is applied for every new reached message. Next import attribute is receive message wait time, this attribute sometimes is referred as long polling. It works as following, whenever there is new process trying to pick a message, when this attribute is setting as 0, if the process finds the queue is empty, it will not wait for the new message arrives, the process will immediately shutdown the connection, in such scenaro, unnecessary connections will be established multiple times which costs extra billing. Therefore, normally we will set the value more than 0.\nDead letter queue dead letter queue is another queue you have to associate with a queue you have created. Which used to store those messages cannot be processed by that queue. For example, you can create another process which has more CPU and memory power to process the message that cannot be processed from the standard queue. What’s more, you might want to use dead letter queue to diagnose why the message cannot be processed in the standard queue. There is only one important attribute, maximum receives, this attribute is used to determine after how many processes tried the message failed, then put the message into the dead letter queue.\nAbout the encryption option, please refer my another article called AWS KMS\nOther attributes when turn on the functionality of content-based deduplication. The duplicated message will be removed automatically by the queues. Message available is the metric that shows amount of messages that you pushed into the queue and Messages in Flight is the total amount of messages that is processing.\nThere are several scenarios a SQS queue can be used, normally combine SQS queue with lambda is a good practice, you can trigger a lambda function whenever there is a new message arrives the queue. Another use case is combining SQS with SNS service to create a fan out architecture. Because, SQS can guarantee the message comes from a unique resource, whereas SNS can response the multiple delivery work. the image above shows such architecture, the leftmost is the SQS service, the middle one is SNS service, suppose you have one message that has to be processed by different services, maybe one service is doing filtering, another service is for storing this message.\n","date":1580688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580688000,"objectID":"b05784b12848a968dc58ed066bb280b6","permalink":"https://terry-pan-dev.github.io/post/aws-sqs/","publishdate":"2020-02-03T00:00:00Z","relpermalink":"/post/aws-sqs/","section":"post","summary":"details about AWS SQS","tags":["aws"],"title":"AWS-SQS","type":"post"},{"authors":null,"categories":["AWS"],"content":" AWS KMS AWS KMS is the service that amozon provided for managing encryption keys.\nBefore really dive into AWS KMS, it’s better to introduce several concepts about encryption. Generally there are two types of encryption, one is symmetric encryption another is asymmetric encryption.\nSymmetric encryption is normally used for local file encryption, as the name indicates symmetric encryption uses one key to encrypt data and using same key to decrypt data, in such case keep this key secret is important.\nsymmetric\nAsymmetric encryption is generally used for transmission scenario, for example keep a secret tunnel to transmit files. Asymmetric encryption has two keys. One key is called private key, another key is called public key. Private key will be always kept in the sender part and never revealed it publicly. Whereas, public key can be transmitted to worldwide. General use case is following\nStep 1: sender will send his/her public key to receiver (actually anyone can have the public key).\nStep 2: receiver is now able to use the public key to encrypt his/her data, then send back to the sender, even there is an eavesdropper, the eavesdropper cannot use his/her public key to decrypt this data. Therefore, data will be kept secret between the real sender and receiver.\nStep 3: when sender gets the encrypt data package, sender uses his private key to decrypt the packge.\nasymmetric\nBoth symmetric and Asymmetric encrytion are used for avoiding information leak to others. There is another important concept called digital signature is also worth to mention before give the detail of AWS KMS. Digital signature is used for keeping data integrity, avoiding data forge, notice it’s different than symmetric and asymmetric above, digital signature is not used to make data secret. However, digital signature utilizes asymmetric encryption. The way that digital signature works by following step.\nStep 1: sender using certain hashing algorithm (md5 for example) to generate a data digest, this digest represents the data, even one byte change, the digest will be different.\nStep 2: sender using his/her private key to encrypt the digest which can be decrypt by public key.\nStep 3: when receiver gets the data and encryptd digest, receiver can decrypt the digest first, then using same hashing algorithm to hash the received data. Afterwards, comparing the generated digest with the received digest, if there is any difference, it means the data has changed by someone.\nNotice, anyone who has the public key can decrypt the sender’s package. However, once they change any information inside the data and using their public key to encrypt the digest. The real receiver will not be able to use his public key to decrypt the encrypted digest, since public key cannot used to encrypt/decrypt data.\ndigital\nNow, it’s the right time to give the detail of AWS KMS. AWS KMS is used to manage symmetric keys, not asymmetric keys. AWS KMS uses a concept called envolope encryption, before introduce this concept, it’s better to understand two other things, one is called master key, another is called data key. Data key is the real encryption key that used to encrypt your data. Whereas, master is used to encrypt your data key, this process makes the encryption more strong. There is one more difference between data key and master key, you can view the data key but you cannot really view the master key, because the master is kept on AWS and managed by AWS, you can use but not view it. The process of envolope encryption is like this, AWS KMS generates data keys which used to encrypt client data, at the same time AWS KMS using master key to encrypt the data keys. Now you use the generated data keys (plain text) to encrypt data with the encrypted data key attached and the plain text data keys will be destroied. When clients want to decrypt the data, they have to use the master key to decrypt the encrypted data key first, after the data has been decrypted, clients can use the decrypted data key to decrypt the data [1].\nkms\nReferences 1. Escrowed data and the digital envelope.\nAbles K, Ryan MD. In: International conference on trust and trustworthy computing. Springer; 2010. pages 246–56.\n","date":1579824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579824000,"objectID":"2d23a2a0d7c0f2c541047f22f566f646","permalink":"https://terry-pan-dev.github.io/post/aws-kms/","publishdate":"2020-01-24T00:00:00Z","relpermalink":"/post/aws-kms/","section":"post","summary":"details about encryption of KMS","tags":["aws"],"title":"AWS-KMS","type":"post"},{"authors":null,"categories":["Data Science"],"content":" Introduction SVM was probably the most powerful classic machine learning algorithm before neural network in practice.\nBefore really dive into SVM, there are several terminologies worth to introduce, one is call hyperplane, it defines as in a p-dimensional space, a hyperplane is a flat affine subspace of dimension p-1 [1], to make it more concrete, in 3-d, the hyperplane is a plane. Whereas, in 2-d, the hyperplane is a line.\nNext concept is called margin, suppose we have a svm plot like following as you can see from the image above, the hyperplane is the dash line in the middle, the margin is the width from the solid line to dash line, we have support vectors to define margin. Which are the closest point to the hyperplane. However, we may not always have such perfect case. Suppose data point are interleaving, we can not find a hyperplane perfectly separate two classes. However, if we can tolerate certain level of misclassifying and find a hyperplane to minimize this error, we call this soft margin.\nMaximal margin classifier I have to give a math definition for later use, to define a hyperplane in math equation \\[ \\beta_0 + \\beta_1X_1 + \\beta_2X2 + \\cdots + \\beta_pX_p = 0 \\] if \\(p=2\\) we have a line, if \\(p=3\\) we will have a plane. Supposing we have a point in p-dimension that satisfies the equation above, we say that this point is sitting on this plane. If not, this point is either in \\[ \\beta_0 + \\beta_1X_1 + \\beta_2X2 + \\cdots + \\beta_pX_p \u0026gt; 0 \\] or \\[ \\beta_0 + \\beta_1X_1 + \\beta_2X2 + \\cdots + \\beta_pX_p \u0026lt; 0 \\] we know that the middle dash line is our decision boundary. Therefore, any point lies on the line equal to zero. Otherwise, not greater than or less than zero. Moreover, the larger the value the longer the distance between the point and the line. Therefore, if you only need decide the margin, we do not need all points, we know need three as above which we call it support vectors. This seems like a greate property, since we do not have to calculate many observations.\nfor people who have used sklearn might notice, the svm classifier in sklearn has a same parameter called C. However, they are opposite each other, the C in sklearn is the penalty term. Which means if you give a big C value, when the classifier misclassifies points, it will get big penalty, in other words, the classifier must make less misclassification to reduce the error or you could say narrowing down the margin and vise versa.\nNon-linear decision boundary suppose we have dataset like following: It’s obviousely that we cannot use linear classifier to make the decision boundary any more. However, what if we can transform the original feature space to higher order? we have an similar example like the right side of above image. It has become linearly separable.\nReferences 1. An Introduction to Statistical Learning [Internet].James G, Witten D, Hastie T, Tibshirani R. New York, NY: Springer New York; 2013 [cited 2019 Nov 8]. Available from: http://link.springer.com/10.1007/978-1-4614-7138-7 ","date":1568332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568332800,"objectID":"9fe1c8ee3777716084d9a94d71269180","permalink":"https://terry-pan-dev.github.io/post/support-vector-machine/","publishdate":"2019-09-13T00:00:00Z","relpermalink":"/post/support-vector-machine/","section":"post","summary":"detailed explanation of SVM","tags":["Machine Learning"],"title":"SVM","type":"post"},{"authors":null,"categories":["Data Science"],"content":" Introduction Although nowadays deep learning is gaining more attention than classic machine learning methods, classic machine learning algorithm is still have its advantage than deep learning cannot beat, like simplicity and interpretability. Among classic machine learning algorithm, decision tree or more accurate its variation like random forest or bagging is widely used in industry. The beauty of decision tree is that it closes to human making decision.\nThere are several terms worth to explain in front. Here is an image to help me to illustrate it Normally, a tabular data is binary splitting into the tree like a bove, we are choosing a feature (colume) to split the data (row), the decision node we call it internal node, then we recursively split the tree until we reach certain threshold for example only two observations in the terminal node, terminal node is the end of a tree, we call it leaf node as well.\nAs you can see above, the first internal node probably the most important factor among other nodes, because based on this decision we decide where to go. Therefore, this nature makes decision easy to interpret. Moreover, each terminal node is actually divided into rectangle as you can see from the left on the image above.\nSo is decision tree able to be used in regression and classification tasks as other machine learning algorithms?\nRegression tree Regression tree can be used in a regression task like linear regression, so how does regression do a prediction? A regression tree predicted value is the mean response of the training observations [1]. To make it simple, when we built our decision tree, the predicted value is just the average value in each terminal nodes or each rectangle. The way to decide the splitting value is by minimize the mean value with each observations in each rectangle. \\[ \\sum_{j=1}^J\\sum_{i\\in R_j} (y_i-\\hat{y}_{R_j})^2 \\] Here, \\(R_j\\) is the rectangle or terminal node. What we have to do is just minimize the total error. When we doing a prediction, we just give a training observation based on decision we find the rectangle it belongs to, then we find the mean value inside that region. So how does a classification task can be done by decision tree?\nClassification tree Classification tree is nothing special than a regression tree, only difference is the error measurement. For classfication problems, we don’t have quantitative ground truth, instead we have qualitative value. Therefore, one measure called Gini index was introduced, Gini index is defined by \\[ G = \\sum_{k=1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk}) \\] this is a measure of total variance across K classes, Where \\(\\hat{p}_{mk}\\) is the proportion of observations in the mth region (rectangle) for kth class. Gini index measures the purity of nodes, or call it impurity [2] why? as you can see the smaller the value the more a node from single class. in other words, this node is not impure. Which more close to human intuition. Another measure is called entropy, it is defined as \\[ E = -\\sum_{k=1}^K \\hat{p}_{mk}log_2(\\hat{p}_{mk}) \\] as you can see, this measurement is close to Gini index measurement. Here I created an artificial dataset to demostrate how to use entropy. Supposing we have 2-class dataset like following\nx1 x2 y 1.5 2.3 0 1.8 3.2 0 1.3 3.3 0 2.1 3.1 1 2.3 3.4 1 2.8 4.0 1 if we split the tree based on \\(x1 \u0026lt;= 1.8\\), the node on the left will purely belongs to class 0, where the right node will all belongs to class 1. This is a better choice against \\(x2\\). Why? because the entropy for the left node is 0, and so as right node. Whereas, if we choose \\(x2 \u0026lt;= 3\\) as our splitting node, we will have entropy for left node is 0, but for right node we have entropy \\((-0.4*log(0.6)) + (-0.6*log(0.6)) = 0.97\\). Which compared to splitting by \\(x1\\) is less optimal.\nHere is a plot to show the differences for different measurement methods [3]\nmeasure\nPruning when you don’t specify the max depth for decision tree, decision tree will lead to overfitting. Pruning is for prevent overfitting, there are two strategies to prun a tree, one is called prepruning, which stop earlier when training a tree. Whereas, another is called postpruning, which allows the tree grow freely then pruning the tree. Each has its one advantage over other, for example prepruning will training fast, whereas, postpruning usually has a good accuracy over prepruning.\nPros, Cons trees are easy to explain to people (+) trees can be displayed graphically (+) trees can easily handle qualitative variable without creating dummy variable (+) no need to center or scale data (+) no as accurate as other regression and classification methods (-) However, by adding more decision trees, the power of decision trees emerge, one representative algorithm is random forest.\nRandom Forest random foreset is an ensemble learning algorithm, as the name shows, it consists many decision trees, it also utilizes a statistical method called bootstrap, bootstrap is a way to sample data from a limited datasets, so that we can have enough dataset to train. random forest is similar like bagging algorithm the difference between bagging algorithm is that random forest using fewer predictor variable (feature) to training. Which will help reduce the correlation between predictor variables. One advantage of random forest is that even add more decision trees, there will not lead to overfitting. There are other algorithms based on decision tree, like Adaboosting, XGBoosting, CART, etc.\nTips Here are some tips from practice [4] regard to sklearn. - consider performing dimension reduction before training model - using depth=3 as an starting tree to get a feel how the tree look like - use max_depth to control size of the tree to avoid overfitting - when using random forest, choose the training feature as sqrt(total feature) - when using boosting, a typical learning rate \\(\\lambda=(0.01,0.001)\\) will be a good start\ndemo for decision tree ### References\n1. An Introduction to Statistical Learning [Internet].\nJames G, Witten D, Hastie T, Tibshirani R. New York, NY: Springer New York; 2013 [cited 2019 Nov 8]. Available from: http://link.springer.com/10.1007/978-1-4614-7138-7\n2. Introduction to machine learning.\nAlpaydin E. 2nd ed. Cambridge, Mass: MIT Press; 2010. 3. Decision trees: An overview and their use in medicine.\nPodgorelec V, Kokol P, Stiglic B, Rozman I. Journal of medical systems 2002;26(5):445–63. 4. The online teaching survival guide: Simple and practical pedagogical tips.\nBoettcher JV, Conrad R-M. John Wiley \u0026amp; Sons; 2016. ","date":1567987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567987200,"objectID":"b70958c73035eb346d6e3ab6701d6372","permalink":"https://terry-pan-dev.github.io/post/decision-tree/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/post/decision-tree/","section":"post","summary":"Detailed decision tree","tags":["Machine Learning"],"title":"Decision tree","type":"post"},{"authors":null,"categories":["Data Science"],"content":" Introduction LSTM stards for Long-short term memory, it is a variant of RNN network. The strong part of LSTM archetect is it has memory build-in. Imaging the daily conversation, we are able to predict next words based on the context of the conversation because we have remembered the important of part of the context. See example below,\nad_pre\nWhat you can remember after several days probabily only a few key information, and you choose to forget irrelavant information. Here is the things you possible remember after several days\nad_post\nAs LSTM can remember through time. Therefore, it has wide range of applications like text/speech translation, speech to text translation, audio/video prediction\nLimitations References ","date":1566950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561713821,"objectID":"12938801e885af9ac324dfb895e4e81e","permalink":"https://terry-pan-dev.github.io/post/lstm/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/post/lstm/","section":"post","summary":"LSTM explanation and exmples","tags":["Machine Learning"],"title":"LSTM","type":"post"},{"authors":null,"categories":["Math"],"content":" Introduction Maximum likelihood is a fairly common optimization tool in machine learning\nThere are many books discussed about MLE. However, the derivation of formulas are often not clear. Thus in this article I am going to disclose of several MLE formulas step by step, hoping that will benefit all machine learning practitioners.\nto be continue…\nReference ","date":1564185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564185600,"objectID":"386a48f88d1dd4489b8b6078a2b45c94","permalink":"https://terry-pan-dev.github.io/post/maximum-likelihood-estimation/","publishdate":"2019-07-27T00:00:00Z","relpermalink":"/post/maximum-likelihood-estimation/","section":"post","summary":"Detailed linear regression","tags":["Machine Learning"],"title":"Maximum likelihood estimation","type":"post"},{"authors":["Terry Pan"],"categories":["Math"],"content":" Introduction Permutation and Combination is an important concept in statistics. The reason is because in order to know the probability of an event. we often have to calculate sample space.\nTwo words are essential regard to permutation and combination, one is order another is replacement. One quick example will be the combination (general meaning) of letters AB. If order matters AB and BA are different, we count them as 2, but if order does not matter, we only count as 1. Next, if we take account of replacement, we have AA, AB, BA, BB, we count to 4 in terms of order matters. However, if we do not allow replacement, we only have AB and BA. Which we can count to 2. The general idea behind it is that when we allow replacement and order matters, there are more combinations (general meaning).\nThe main differece between permutation and combination is that permutaion order matters whereas combination does not.\nAnatomy Example 1 two letters are choosen with from the word PING, what are the sample space? We can create a table to cover the four cases\ncase replacement order outcome calculation 1 0 0 (PI),(PN),(PG),(IN),(IG),(NG) 3+2+1=6 2 0 1 (PI),(PN),(PG),(IP),(IN),(IG),(NP),(NI),(NG),(GP),(GI),(GN) 3+3+3+3=12 3 1 0 (PP),(PI),(PN),(PG),(II),(IN),(IG),(NN),(NG),(GG) 4+3+2+1=10 4 1 1 (PP),(PI),(PN),(PG),(II),(IP),(IN),(IG),(NN),(NP),(NI),(NG),(GG),(GP),(GI),(GN) 4x4=16 When we have the term replacement, it’s like we have a bag of balls. We can draw a ball then put it back to the bag then draw another one. In terms of example above, we can have the combination with itself. For term order, when order matters, we are able to go backwards, like when we try to find the combination for letter I, we are able to go backwards to letter P, which we have IP. Whereas, when order does not matter, we can only move forward.\nit seems there are centain patterns in the calculation like 3+2+1 and 4+3+2+1. If there are certain patterns, we can find formulas to make calculation easier.\nFormulas the formulation for permutation \\[ ^{n} P_{k} = \\frac{n!}{(n-k)!} \\] we read this as n choose k\nand the formula for combination is \\[ \\left(\\begin{array}{l}{n} \\\\ {k}\\end{array}\\right)=n C_{k}=\\frac{n !}{k !(n-k) !} \\]\nLet’s calculate the result of above question by formulas. First of all, permutation\n\\[ \\frac{4!}{(4-2)!} = 4\\times 3=12 \\]\nthe result matches the calculation above which is without replacement but order matters (case 2), then combination\n\\[ \\frac{4!}{2!(4-2)!}=\\frac{4\\times 3}{2}=6 \\]\nWhich also matches our case 1.\nWhat is the formulas for case 3 and case 4? Which we take account of replacement. for case 3 we have \\[ ^{n} C_{r}=\\frac{(n+r-1) !}{r !(n-1) !} \\]\nLet’s further verify it \\[ \\frac{(4+2-1)!}{2!(4-1)!} = \\frac{5!}{2!3!}=\\frac{5\\times 4}{2}=10 \\]\nit matches our result. Last one is permutation with replacement. \\[ ^{n} P_{r}=n^{r} \\]\nSummary Here is a summary table for the result\ncase replacement order formula combination 0 0 \\(\\left(\\begin{array}{l}{n} \\\\ {k}\\end{array}\\right)=n C_{k}=\\frac{n !}{k !(n-k) !}\\) permutation 0 1 \\(^{n} P_{k} = \\frac{n!}{(n-k)!}\\) combination 1 0 \\(^{n} C_{r}=\\frac{(n+r-1) !}{r !(n-1) !}\\) permutation 1 1 \\(^{n} P_{r}=n^{r}\\) as you can see above, when order is 0, we have combination. Whereas, when order is 1, we have permutation. This is the difference between permutation and combination we have discussed on section 1.\n","date":1564185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564185600,"objectID":"7b7e928c9f9162a29d3c0ce8c78cfaea","permalink":"https://terry-pan-dev.github.io/post/permutation-combination/","publishdate":"2019-07-27T00:00:00Z","relpermalink":"/post/permutation-combination/","section":"post","summary":"Introduction Permutation and Combination is an important concept in statistics. The reason is because in order to know the probability of an event. we often have to calculate sample space.\nTwo words are essential regard to permutation and combination, one is order another is replacement. One quick example will be the combination (general meaning) of letters AB. If order matters AB and BA are different, we count them as 2, but if order does not matter, we only count as 1.","tags":["permutation and combination","math","statistics"],"title":"Permutation and combination","type":"post"},{"authors":["Terry Pan"],"categories":["Software Engineering"],"content":" Introduction Git is a version control system, the author Linus Torvalds is very famous. He is famous by anther system called Linux. Someone may not know him, but I suppose everyone knows Linux. Git is like a state machine which tracks what you have done and makes things safe. When you regret you can go back to history.\nKey concepts The state concept of git is essential for every developer. Here is an image I take from git official website.\ngit\nThe three states are modified, staged and committed. Modified means you have modified files or added new files but have not been using command add to add those changes in stage. After you git add files, those files are meant staged. Afterwards, you are ready to commit. committed means your data is safely stored in your local database. You can find those commits whenever you want as like checking database.\nLet us give several examples. command git status is often use to check status. Suppose you have not modified anything in your git repository.\ngit status the output will be\nnothing to commit, working tree clean. What if you modified somthing?\nChanges not staged for commit: (use \u0026quot;git add \u0026lt;file\u0026gt;...\u0026quot; to update what will be committed) (use \u0026quot;git checkout -- \u0026lt;file\u0026gt;...\u0026quot; to discard changes in working directory) modified: sample.txt no changes added to commit (use \u0026quot;git add\u0026quot; and/or \u0026quot;git commit -a\u0026quot;) Notice, the word from working tree clean becomes changes not staged. Now you can stage the modified files.\n# add all modified files git add . The output is\nChanges to be committed: (use \u0026quot;git reset HEAD \u0026lt;file\u0026gt;...\u0026quot; to unstage) modified: sample.txt The information git given is quite friendly, it tells you that changes to be committed. Now you are ready to commit your staging files. After committed, your modification is meant safe.\ngit commit -m \u0026quot;first commit\u0026quot; after committed, if you issue git status you’ll notice that the message becomes working tree clean again. This is a cycle, we can repeat this cycles as many times as we want. So where is the record of our modifications? using git log to check the commits we have done.\nRegret medicine It’s fairly common that we make mistakes when commit or push.\nScenario 1\nbefore staging. Which means you have modified something but have not add to stage. If you want to discard the change, you can do\n# for specific file git checkout -- filename # for all modified files git checkout -- . Scenario 2\nYou have already add to the stage. However, you found something wrong with it, you want to discard the add action.\n# for specific file git reset HEAD filename # for all staging files git reset HEAD . Notice, what you have done above will not affect what you have modified. The action just put files from staging to workspace. Your modification is still there.\nScenario 3 You have commit the modification. However, you found this is not a proper commit. You can use following command to go back the previous commit.\ngit reset --hard HEAD~ This action completely discard what you have modified. However, if we still want to keep the changes but discard current commit?\ngit reset --soft HEAD~ adding extra argument soft will still keep your changes and if you check the status of git you will find that your modified files are in staging area.\nIf without any arguments attached, the default one is mixed. Which similar like soft, your changed files will still there.\nTips and tricks Alias A developer has to use git frequently. Therefore, sometimes, certain git commands seem too lengthy. The way to overcome this is to create alias. There are three ways to create alias.\nFirst, you can set your alias in bashrc file, the syntax is same as set other alias.\nalias gst=\u0026#39;git status\u0026#39; Or git provides alias functionality as well, for example\ngit config --global alias.co checkout git config --global alias.st status If you use zsh instead of bash. Zsh provides a plugin call oh-my-zsh, it has git extension which has many git alias created for you.\nAdd/delete features In a rapid development environment, business requirements are often unstable. You may have to implement certain feature but finally is discarded. Clients may change their mind and you have to retrieve the feature again. There are two useful commands in git are used to tackle these two issues, git revert and git cherry-pick.\nSuppose you have branch like following: What if you don’t need feature B? delete B? it seems not a good idea, beacuse you may need the feature B later on as we mentioned above. Moreover, since git is a version control system we want to record history not modify it. Here we can use following command to discard feature B.\ngit revert B then you can get a new commit which only contain feature A and C. However, if your manager tell you, you have to add feature B back in the feature, what do you do? write again? not a good idea. Then we have command called git cherry-pick. We can utilize this command to retrieve history back. First, use git log find the sha1 of the feature you want to retrieve back, then using following command\ngit cherry-pick B-sha1 Now we have the feature B back. Both actions did not change the history. Nice and cool!\n","date":1563321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563321600,"objectID":"f6f1565d80621a1191dd712571342de0","permalink":"https://terry-pan-dev.github.io/post/git-flow/","publishdate":"2019-07-17T00:00:00Z","relpermalink":"/post/git-flow/","section":"post","summary":"serveral tips and tricks for git","tags":["git","git flow","github flow","git tips","git tricks"],"title":"Tips and tricks for git","type":"post"},{"authors":["Terry Pan"],"categories":["javascript"],"content":" Introduction Javascript is a single thread language by default. However, as web world moves fast, a single thread cannot undertake heavy tasks. It’s hard to change the fundamental mechanism from a single thread to multi-threads. To solve this problem, people come up with the concept of event loop. The concurrency model of javascript is based on event loop [1]\nEvent loop Prior to explaining the event loop. It’s better to understand something called block I/O and non-block I/O. Suppose the application you are coding is a single threaded application, whenever you send a request to the Internet or read files from the local file system. It blocks, which means the rest of code can only be running after the request finished. for example:\nimport requests // this is i/o request val = requests.get(\u0026#39;http://example.com\u0026#39;) print(val) requests.get(\u0026#39;http://example2.com\u0026#39;) renderPage() Line 3 will run first, depends on the network, we may wait milliseconds or seconds, before the fetch finish, the rest code cannot be executed, which obviously is inefficient, because code print(val) may depends on the result above. However, the rest code does not, but you still have to wait until the request finished to execute the remaining code.\nSo how does event loop work? Here is an image shows the fundamental concept: To make it simple, every time when you do any I/O task, this task will be set as an event and put inside an event queue then the rest code will be executed as normal. After you run out all your functions in your call stack. Event loop will come to the event queue to check if there any job has not been done, if so, event loop will fetch the event inside the queue based on priority (macro and micro event, we will talk it in the next section). There is a event loop visualizer created by Philip Roberts [2].\nTo make it concrete, here is an example code.\nsetTimeOut(function(){ console.log(\u0026quot;event 1\u0026quot;); }, 1000); setTimeOut(function(){ console.log(\u0026quot;event 2\u0026quot;); }, 2000); both setTimeOut will be put inside an event queue, waiting to be executed. Event 1 will be executed after 1000 milliseconds (1 second), event 2 will be executed after 2000 milliseconds (2 seconds). Actually, it is not precisely 1 second or 2 seconds, we will discuss the reason later on.\nMacro task vs Micro task Have a look following code, think about what will be printed\nsetTimeout(function(){ console.log(1); }, 0); new Promise(function(resolve, reject){ console.log(2); resolve(3); console.log(4); }).then(function(value){ console.log(value); }) console.log(5); ## 2 ## 4 ## 5 ## 3 ## 1 The output is surprise right? Let’s analyze it,\nBoth setTimeout and Promise are events so that they will be executed asynchronously. However, since promise is a micro task which has higher priority than setTimeout (macro task), even we have setTimeout is 0, promise will run first, the code inside promise will run first, therefore 2 is printed first, resolve is a callback function, normally resolve will wait for the result of an async call. Therefore, the code will continue running to print 4 and return a promise back then we have console.log(5) which will print number 5. Afterwards, the callback of promise running which prints number 3, finally number 1 will be printed.\nJavascript event has two categories\nMacro task Micro task (higher priority) By default, when the single main thread encountering functions (sync), it will put functions inside call stack, execute them then pop them out, if the function running for a long time without returning value, the main thread has to wait there. This is why single thread not effcient. However, javascript will make the time consuming task (I/O) return a placehold value first. The I/O task will be send to browser kernel, when task finished, task and its callback functions will be send back to task queue. The advantage of this strategy is that the main thread is able to continue running without blocking, after the main thread run out off all the functions that inside the call stack, the main thread will ask the task queue for more jobs to do, once there has any task, it will be pushed into the call stack execute, pop up. This kind of steps will be running in a loop, that’s why we call it event loop.\nMacro task\nSetTimeout SetInterval SetImmediate ajax Micro task\nasync/await promise Let’s do a crazy test to see if we really understand event loop. What will be printed the following code?\nasync function async1() { console.log(\u0026#39;async1 start\u0026#39;); await async2(); console.log(\u0026#39;async1 end\u0026#39;); } async function async2() { console.log(\u0026#39;async2\u0026#39;); } console.log(\u0026#39;script start\u0026#39;); setTimeout(function() { console.log(\u0026#39;setTimeout\u0026#39;); }, 0) async1(); new Promise(function(resolve) { console.log(\u0026#39;promise1\u0026#39;); resolve(); }).then(function() { console.log(\u0026#39;promise2\u0026#39;); }); console.log(\u0026#39;script end\u0026#39;); ## script start ## async1 start ## async2 ## promise1 ## script end ## async1 end ## promise2 ## setTimeout So let’s demystify the code above, first of all, async function is essentially return a promise. However, async1 and async2 are just defined here not called. Therefore script start print first. Then setTimeout is another async function which has lower priority. Afterwards, async1() is called, async1 start print first, async2() is another promise, the code inside will be printed, hence, async2 was printed. Next, we have another promise, this time promise1 was printed, since resolve() is a callback, the code will continue running. script end prints next. Let’s trace back what we have left, async1() has not finished yet. Therefore, async1 end will be printed and our callback of promise has not finished yet, promise2 will be printed. Finally, the macro task setTimeout will be printed.\nReference 1. Concurrency model and Event Loop [Internet].MDN Web Docs2019;Available from: https://developer.mozilla.org/en-US/docs/Web/JavaScript/EventLoop\n2. 2014;Available from: http://latentflip.com/loupe\n","date":1562803200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562803200,"objectID":"20588e04e4fd9e7a7fb7d42aec8130c4","permalink":"https://terry-pan-dev.github.io/post/js-event-loop/","publishdate":"2019-07-11T00:00:00Z","relpermalink":"/post/js-event-loop/","section":"post","summary":"dissect javascript event loop","tags":["javascript","event loop","event queue"],"title":"Javascript event loop explained","type":"post"},{"authors":["Terry Pan"],"categories":["javascript"],"content":" Introduction The scope and context of javascript maybe the most bizarre point when learning javascript, for people who learn programming from C/C++/JAVA those traditional language will find the concept of scope and context are particularly difficult to understand. However, it’s essential for a javascript developer to understand these.\nGlobal scope When you starting write a javascript, there is a default global scope called Window. Have a look at the image below\nglobal\nWhen a variable is in global scope it can be access by any other scope\nvar name = \u0026quot;Albert Einstein\u0026quot;; console.log(\u0026quot;1: \u0026quot;, name) function print(){ console.log(\u0026quot;2: \u0026quot;,name); (function inner(){ console.log(\u0026quot;3: \u0026quot;, name); function inner2(){ console.log(\u0026quot;4: \u0026quot;, name); } inner2(); })(); } print(); ## 1: Albert Einstein ## 2: Albert Einstein ## 3: Albert Einstein ## 4: Albert Einstein As you can see above, it prints four names. the first print is in the global scope, the second print is in the print function scope and so for print 3 and print 4.\nlet vs var\nLet’s discuss local scope or function scope, as we mentioned above, global scope can be accessed everywhere, but local scope can only be accessed locally. In ECMAScript 6, there are two keywords introduced. Which will make the variable defined locally. let is just like var with local scope restriction and as the name shows const is a constant with local scope, when you try to modify a constant variable the interpret will yelling.\nif(1){ // global scope var name = \u0026quot;Albert Einstein\u0026quot;; // local scope let age = 140; // local scope const occupation = \u0026quot;scientist\u0026quot;; } console.log(name); // can be accessed console.log(age); // cannot be accessed console.log(occupation); // cannot be accessed Context When we talking about context, we are referring this keyword. Here is a definition from MDN [1]. \u0026gt; In most cases, the value of this is determined by how a function is called\nSo, what we have to find is who called this function. If a object called a function then this is the object, if a button is called the function, then this is the button. Let’s see several examples\ncalling from window\nconsole.log(this); // Window {postMessage: ƒ, blur: ƒ, focus: ƒ, close: ƒ, parent: Window, …} calling from an object\nvar obj = { func: function(){ return this; } }; console.log(obj.func());// return an object {} strict mode if we don’t defind use strict in the beginning of a javascript document. this keyword will be default set to window in browser and global in nodejs.\nwithout strict mode set\nfunction foo(){ return this; } foo() === window; // true setup strict mode\nfunction boo(){ \u0026#39;use strict\u0026#39;; return this; } boo() === undefined; // true It’s always a good practice to set the strict mode\nApply, Call, Bind\nThese three functions are quite useful, apply and call are similar. bind is used to bind a function to another context. Here is an example showing that how people can change the context to make the calling more dynamic\nvar test = { prop: 42, func: function() { return this.prop; }, }; var test2 = { prop: \u0026#39;Albert Einstein\u0026#39; } console.log(test.func()); // print 42 console.log(test.func.call(test2)); // print Albert Einstein like a magic right? Here is the different use of apply, call and bind\nfunc.call(context, \u0026quot;arg1\u0026quot;, \u0026quot;arg2\u0026quot;, \u0026quot;argn\u0026quot;); func.apply(context, [\u0026quot;arg1\u0026quot;, \u0026quot;arg2\u0026quot;, \u0026quot;argn\u0026quot;]); func.bind(context); call has slighly faster in performance than apply [2]\nPublic and Private\nThe function scope can be used to mimic traditional OOP language properties like private variables. Moreover, it can be used to manage namespace.\n// only expose AwesomeModule keyword globally var AwesomeModule = (function(){ // private variable and methods let property = \u0026quot;property\u0026quot;; function privateFunc(){ ... } // you can access the method by AwesomeModule.publicFunc() // this kind format more like OOP return { publicFunc: function(){ // can access property and privateFunc() } } })() the code above has a small bizarre format (function() {})(). This is called immediately invoked function expression (IIFE). How to interpret this code? It’s simple, you define a function like this function(){}, you want to invoke it after define it the you just add an parenthesis (). However, in order to define this function you have to use another parenthesis enclose them, that’s all.\nClosure Closure is quite useful, like another magic in javascript. We know when a function return the variables define in that scope will be destroied or collected, but closure is not working that way, even the function returned, closure can still keep the variable. Let’s see an example\nfunction outer(outerArg){ let property = \u0026quot;property\u0026quot;; return function(innerArg){ console.log(outerArg); console.log(property); console.log(innerArg); }; } // notice outer has returned, property should be destroied // but closure not working that way var func = outer(\u0026quot;from outer function\u0026quot;); // what we returned is not a variable is a function, we have to invoke the function func(\u0026quot;from inner function\u0026quot;); // output // from outer function // property // from inner function The example above shows that closure can not only access its own arguments, but the arguments from its outer functions and the returned function will not be invoked immediately, you can invoke the returned function whenever you want. Which is really handy. There is a very good visualization tool created by Tyler McGinnis. Which gives a really concrete example of what closure is [1]\nReferences 1. The Ultimate Guide to Hoisting, Scopes, and Closures in JavaScript [Internet].TylerMcGinnis.com2019;Available from: https://tylermcginnis.com/ultimate-guide-to-execution-contexts-hoisting-scopes-and-closures-in-javascript\n2. Understanding Scope in JavaScript [Internet].\nAhmed H. Scotch2019;Available from: https://scotch.io/tutorials/understanding-scope-in-javascript\n","date":1562630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562630400,"objectID":"082e808ab23037bab893957387084870","permalink":"https://terry-pan-dev.github.io/post/js-scope-context/","publishdate":"2019-07-09T00:00:00Z","relpermalink":"/post/js-scope-context/","section":"post","summary":"Learn the bizarre points of javascript scope and context","tags":["javascript","context","scope"],"title":"Javascript scope and context","type":"post"},{"authors":[],"categories":["Software Engineering"],"content":" Introduction Docker is a light weight virtual machine, it does not have the OS level abstraction like VMWare or Parallel. Therefore, it is light and resource saving. The main issue it wants to tackle it is the pain of deploying service in different systems. Like developers develope and test projects in their own system but when deploy in the remote server, it may not work.\nImages vs containers Images are similar as the concept class in object oriented programming. It can be considered as a blueprint. Users can create their own images based on others this is just like inheritance in OOP. Whereas, containers just like the concept of object in OOP, we can create multiple containers from one image, suppose each image is just a blueprint of a server, the advantages of this feature make expand server much easier. That’s why container technology is a hot topic in distributed system.\nUseful docker commands docker ps -a docker inspect youthful_elion Here is a image shows the differences between Docker and OS-based virtual machine to be continue… ### References\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"b082a78ed5d941e22b0d73ef202dda1c","permalink":"https://terry-pan-dev.github.io/post/docker/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/post/docker/","section":"post","summary":"Docker","tags":["Docker"],"title":"Docker","type":"post"},{"authors":null,"categories":["Data Science"],"content":" Introduction Linear regression is probably the first topic that every machine learning course will talk. It is simple and often prodivde and adequate and interpretable description of how the inputs affect the output [1]. I will try my best to explain linear regression in a simple way but also with depth insight. For those who scare of math, it is okey just give a glance.\nLet’s start from a very simple equation from high school.\n\\[ y = ax + b \\] Here y is outcome or sometimes we will written as \\(f(x)\\), it is the same thing but written in a different format. a is the slope and b is the intercept. Let’s giving a plot.\nThis example is set intercept to 1.4 and slope as 0.5. For this example, we only have one input (x) and one output (y)\nIn real world application, we normally have more than one feature. Suppose we have an input vector \\(X^T=(X_1, X_2, \\dots, X_p)\\)\nTable 1: A table for advertising dataset X TV Radio Newspaper Sales 1 230.1 37.8 69.2 22.1 2 44.5 39.3 45.1 10.4 3 17.2 45.9 69.3 9.3 4 151.5 41.3 58.5 18.5 5 180.8 10.8 58.4 12.9 Here the input features we could think as X1=TV, X2=Radio and X3=Newspaper. and the y is Sales\nWe have a simple model for multiple features\n\\[ f(X)=\\beta_{0}+\\sum_{j=1}^{p} X_{j} \\beta_{j} \\]\nLet’s plot a regression model for iris dataset to see what exactly going on.\nggplot(df, aes(TV, Sales)) + geom_point() + geom_smooth(method = \u0026#39;lm\u0026#39;, formula = y~x) What we like to do is reduce the residuals.\npng\nSo finally, what we want to do is just minimize the residual sum of squares, which is the equation following: \\[ \\begin{aligned} \\operatorname{RSS}(\\beta) \u0026amp;=\\sum_{i=1}^{N}\\left(y_{i}-f\\left(x_{i}\\right)\\right)^{2} \\\\ \u0026amp;=\\sum_{i=1}^{N}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} x_{i j} \\beta_{j}\\right)^{2} \\end{aligned} \\]\nTo make the formula clear little bit. We could using vectorization or vector notation to represent it.\n\\[ \\operatorname{RSS}(\\beta)=(\\mathbf{y}-\\mathbf{X} \\beta)^{T}(\\mathbf{y}-\\mathbf{X} \\beta) \\]\nlet’s take the derivative respect to \\(\\beta\\). \\[ \\frac{\\partial \\mathrm{RSS}}{\\partial \\beta}=-2 \\mathbf{X}^{T}(\\mathbf{y}-\\mathbf{X} \\beta) \\]\nBecause, if we check the RSS equation above it is a quadratic function. Therefore, this equation is always having a global minimal value. Let’s just set the derivative equal zero, then find what the \\(\\beta\\) is.\n\\[ \\begin{aligned} \u0026amp;-2 \\mathbf{X}^{T}(\\mathbf{y}-\\mathbf{X} \\beta)=0 \\\\ \u0026amp;\\mathbf{X}^{T}(\\mathbf{y}-\\mathbf{X} \\beta)=0 \\quad (1)\\\\ \u0026amp;\\mathbf{X}^{T}\\mathbf{y}-\\mathbf{X}^{T}\\mathbf{X} \\beta=0 \\quad (2) \\\\ \u0026amp;\\mathbf{X}^{T}\\mathbf{y}=\\mathbf{X}^{T}\\mathbf{X} \\beta \\quad(3)\\\\ \u0026amp;(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}=(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{X} \\beta \\quad (4)\\\\ \u0026amp;(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y} = \\beta \\quad (5) \\end{aligned} \\]\nLet’s break it down, the first step we just divide both side by -2, hence we can eliminate the constant. Second step we just apply distribution rule, the third one is to make the equation equal, we just move \\(-X^tX\\beta\\) to the right side. Step 4 is just using a little trick to make \\(X^TX\\) to an identity matrix by multiply \\((X^TX)^{-1}\\), finally, we get \\(\\beta\\).\nIt is obviously that if we multiply \\(X\\) to the \\(\\beta\\) we can calculate the estimated \\(\\hat{y}\\) value\nCase study It is important for an analyst to understand the output of a linear model. It is not just blindly using some packages without thinking. Here I take an example from book An Introduction to Statistical Learning [2].\nLet’s start from single variable linear regression model, then try to give a short explanation to the output. Here we use Boosten dataset from R package of MASS, this dataset records 506 neighborhoods around Boosten, there are several feature columns, like crim indicates per capita crime rate by town, rm means the average number of rooms per dwelling and ptratia means pupil-teacher ratio by town.\nHere we use feature lstat (lower status of the population) to predict medv(median value of owner-occupied homes in $1000).\nlm.fit \u0026lt;- lm(medv ~ lstat, data = MASS::Boston) summary(lm.fit) ## ## Call: ## lm(formula = medv ~ lstat, data = MASS::Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.168 -3.990 -1.318 2.034 24.500 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 34.55384 0.56263 61.41 \u0026lt;2e-16 *** ## lstat -0.95005 0.03873 -24.53 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 6.216 on 504 degrees of freedom ## Multiple R-squared: 0.5441, Adjusted R-squared: 0.5432 ## F-statistic: 601.6 on 1 and 504 DF, p-value: \u0026lt; 2.2e-16 Let’s interpret the summary one by one. The first is the call field, it is just simply the formula of this model, nothing special. Next one is the statistical summary for residuals, actually we can use box plot to show it more concretely.\nggplot(data.frame(resi = residuals(lm.fit)), aes(x = \u0026quot;\u0026quot;, y = resi )) + geom_boxplot() As we can see, the plot above matches the summary. The middle thicker horizontal line is the median (-1.318) in the summary, and the line above and below the median line is the first quantile (1Q) and third quantile (3Q). The points on the top and bottom is the min and max value.\nThe next important field is Coefficients, by observing this field, we can find the how predict variables affect the response variable, this is why linear regression model is an interpretable model.\nIf you recall from the beginning of this blog, there is a formula \\(y = ax + b\\) here the intercept is 34.55384, we can think this value is as \\(b\\) in the formula and \\(a\\) is lstat which is -0.95005. Just by these information, we could know that lstat and medv is negatively related, which means when lstat increase 1 unit, medv will decrease -0.95005. Std. Error here can be use to calculate the confidence interval and t value is used to calculate p-value. Therefore, if we really want to know the confidence interval for coefficients, we can use function confint in R.\nconfint(lm.fit) ## 2.5 % 97.5 % ## (Intercept) 33.448457 35.6592247 ## lstat -1.026148 -0.8739505 The confidence interval just like asking people to guess something, suppose ask a veteran mechanics what is the probability that a certain engine will be broken, he will give his experienced guess but with amount of uncertainty, but if you ask an amateur, he will also give his guess but with a more large range of uncertainty.\nJust by knowing the confidence interval you could know that if a coefficient significant of not. When there is a 0 appears between the confidence interval you could confirm that this coefficient is not significant, because there may have a chance the coefficient becomes 0. Which makes that input feature no meaning at all.\nAfter building the model, we can use this model to predict new coming data\nlm.pred \u0026lt;- predict(lm.fit, data.frame(lstat=(c(5,10,15))), interval = \u0026quot;confidence\u0026quot;) lm.pred ## fit lwr upr ## 1 29.80359 29.00741 30.59978 ## 2 25.05335 24.47413 25.63256 ## 3 20.30310 19.73159 20.87461 the test data is confirming our interpretation above, as lstat increasing medv decreasing. Notice, here we use a special parameter, interval=“confidence”. this parameter will give the prediction around the mean of the prediction. To interpret the result above, with 10 lstat on average the value for medv is between 24.47 and 25.63.\nWe can set the parameter inverval as “prediction” as well. However, this prediction only give the prediction interval around a single value. This setting is highly rely on residuals are normally distributed. I will give the methods how to test the normality of residuals in the caveat section.\nCaveats Linear regression model has its limitation or more precisely saying it has assumption For example, as the name indicates, there must be a linear relationship between predict variables and response variable, because the solution will either line plane or hyperplane which is not curvy. I list several assumptions regard to linear regression.\nresiduals are roughly normally distributed residuals are independent linear relation between features and output no or little multicollinearity homoscedasticity (residuals agains fitted value should keep constant) sensitive to outliers There are several ways to test the assumptions\nresidual normality By checking residuals normality, we can use Q-Q plot,\nlm.fit \u0026lt;- lm(medv ~ rm + lstat + crim + dis, data = Boston) ggqqplot(residuals(lm.fit)) How to interpret this plot? Suppose the residuals are following a normal distribution, the points above should be roughly located around the slope line and should be with inside the 95% confidence curvy dashed line.\nIn a algorithmic way to check, we could utilize Shapiro-Wilk test [3].\nshapiro.test(residuals(lm.fit)) ## ## Shapiro-Wilk normality test ## ## data: residuals(lm.fit) ## W = 0.91081, p-value \u0026lt; 2.2e-16 As the result p-value indicates (\u0026lt;0.05). We can reject the null hypothesis (normally distributed). Therefore, the residuals is not normally distributed. Here we can see, it is important to do both visual and algorithmic test, the plot above shows us a roughly normal distributed data. However, the test indicates this is not a normally distributed data.\nresiduals independency By plotting the residuals along with the order of \\(Y_i\\) we can measure the residuals independency.\nplot(sequence(nrow(Boston)), residuals(lm.fit)) abline(h=0, col=\u0026#39;red\u0026#39;) Independent residuals should be located around 0 and randomly scattered.\nlinearity linearity is just checking if the predict variables (input X) has the linear relation between the response variable. After all, we use linear regression to create model, if there is no linear relation we have to use other non-linear model.\ncrPlots(lm(medv ~ rm + lstat + crim + dis, data = Boston)) what we most case is if there is any pattern inside the plot\nsummary(gvlma::gvlma(x = lm.fit)) ## ## Call: ## lm(formula = medv ~ rm + lstat + crim + dis, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.006 -3.099 -1.047 1.885 26.571 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 2.23065 3.32214 0.671 0.502 ## rm 4.97649 0.43885 11.340 \u0026lt; 2e-16 *** ## lstat -0.66174 0.05101 -12.974 \u0026lt; 2e-16 *** ## crim -0.12810 0.03209 -3.992 7.53e-05 *** ## dis -0.56321 0.13542 -4.159 3.76e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.403 on 501 degrees of freedom ## Multiple R-squared: 0.6577, Adjusted R-squared: 0.6549 ## F-statistic: 240.6 on 4 and 501 DF, p-value: \u0026lt; 2.2e-16 ## ## ## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS ## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM: ## Level of Significance = 0.05 ## ## Call: ## gvlma::gvlma(x = lm.fit) ## ## Value p-value Decision ## Global Stat 638.73 0.000e+00 Assumptions NOT satisfied! ## Skewness 143.63 0.000e+00 Assumptions NOT satisfied! ## Kurtosis 289.67 0.000e+00 Assumptions NOT satisfied! ## Link Function 175.67 0.000e+00 Assumptions NOT satisfied! ## Heteroscedasticity 29.76 4.902e-08 Assumptions NOT satisfied! hemoscedasticity For checking hemoscedasticity. Which means the variance of residuals should be constant, see the plot below.\npar(mfrow=c(2,2)) plot(lm.fit, pch = 16, cex = 0.5) We are intereted in the left side two plots. To express homoscedasticity in a plot, it would look like a roughly flat line with random points around it. However, in our case here [4]. We could find the red line is curvy and the points have a growth trend from left to right.\nIf you think visualization is not enough to determine if a model is hemoscedasticity, There are several algorithmic ways to test as well.\ncar::ncvTest(lm.fit) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 0.4130551, Df = 1, p = 0.52042 By checking the p-value, we could see that is significant (\u0026lt;0.05) enough to reject the null hypothesis (variance is constant).\noutliers Because we use RSS to estimate the accuracy, it is obviousely that outliers will highly influence the RSS. Which makes the wrong interpretation to the model.\nA data point has high leverage, if it has extreme predictor x values [5]. Let’s using Residuals vs Leverage plot\npar(mfrow=c(1,2)) plot(lm.fit, 4) plot(lm.fit, 5, pch = 16, cex = 1) The plot above shows that #23, #49 and #39 are influenced points. Among those three points, #23 and #49 are close to 3 standard deviation. Which is more influential than #39. However, are these outliers really influence the result of the regression analysis? Here we need one metric called Cook’s distance to check if data points really influential.\nA rule of thumb is that an observation Cook’s distance exceeds \\(4/(n-p-1)\\), where \\(n\\) is the number of observations and \\(p\\) is the predictor variables. Thus, we can calculate Cook’s distance for this model, which is 4/(50-1-1) = 0.08. We can add a horizontal line to the Cook’s distance plot\nolsrr::ols_plot_cooksd_bar(lm.fit) As we can see above, data point #23 and #49 are really influence the result of linear regression analysis.\nReferences 1. The elements of statistical learning.\nFriedman J, Hastie T, Tibshirani R. Springer series in statistics New York; 2001. 2. An introduction to statistical learning.\nJames G, Witten D, Hastie T, Tibshirani R. Springer; 2013. 3. Shapiro-Wilk Test [Internet].\nLohninger H. 2012;Available from: http://www.statistics4u.info/fundstat_eng/ee_shapiro_wilk_test.html\n4. How to detect heteroscedasticity and rectify it? [Internet].R-bloggers2016;Available from: https://www.r-bloggers.com/how-to-detect-heteroscedasticity-and-rectify-it\n5. Linear Regression Assumptions and Diagnostics in R: Essentials - Articles - STHDA [Internet].2019;Available from: http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials\n","date":1561680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561713821,"objectID":"e6e6efdc5637a0fbd0848add3fa13557","permalink":"https://terry-pan-dev.github.io/post/linear-regression/","publishdate":"2019-06-28T00:00:00Z","relpermalink":"/post/linear-regression/","section":"post","summary":"Detailed linear regression","tags":["Machine Learning"],"title":"Linear Regression","type":"post"},{"authors":null,"categories":null,"content":"","date":1554940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554940800,"objectID":"de447090eb0e256ddf26c67686224b31","permalink":"https://terry-pan-dev.github.io/project/draw2data/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/project/draw2data/","section":"project","summary":"a light tool for generating random datasets by drawing","tags":["tools"],"title":"draw2data","type":"project"},{"authors":["Terry Pan"],"categories":null,"content":" Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://terry-pan-dev.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nA fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://terry-pan-dev.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Terry Pan"],"categories":["javascript"],"content":"Javascript has been dramatically used in front-end and back-end compared to the time when it was invented. When the time it was invented, it supposes to be a joy programming language, doing some simple interactions between browser. However, since the ubiquitous usage of browser. Javascript has become the most important language in the web ever.\nUnlike language like Java and Python. Javascript does not have a module loader initially. Here are several ways to load module from past to present.\nClassic RequireJs(AMD) CommonJs ES6 Old Fashion In the past, when javascript was still not heavily used. There are probably only several js files. It\u0026rsquo;s easy to manipulate it manually. Here is an example.\n\u0026lt;script src=\u0026#34;js/jquery.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;js/point.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;js/line.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;js/polygon.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;js/main.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Since there are many modules/files rely on jQuery, if you put jQuery script tag at the end of this file, the modules which rely on jQuery library will not work just like building a house you need build basement first, then each level up or like draw an image, you need point, line and polygon first. The disadvantage of this kind of module loader is you have always take care of them, once you put them in a wrong order, the console will yell. If your application is small, that\u0026rsquo;s okey, you can well manipulate them manually. Suppose this is a big project which involves many developers, to control the dependency manually will be a nightmare.\nRequireJs Here is a paragraph I get from requirejs homepage.\nRequireJS is a JavaScript file and module loader. It is optimized for in-browser use, but it can be used in other JavaScript environments, like Rhino and Node. Using a modular script loader like RequireJS will improve the speed and quality of your code.\nAs this quote mentioned requirejs is a module loader, it can be used both in front-end and back-end. It\u0026rsquo;s quite simple to use it. Suppose you have a document file tree like this.\n- project_directory/ - index.html - scripts/ - main.js - utils.js - someutils.js The main.js is your main entry point, every code should be traced back to main.js, unlike old fashion, you don\u0026rsquo;t have to create many script tags. All you need to do is like following.\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My Sample Project\u0026lt;/title\u0026gt; \u0026lt;!-- data-main attribute tells require.js to load scripts/main.js after require.js loads. --\u0026gt; \u0026lt;script data-main=\u0026#34;scripts/main\u0026#34; src=\u0026#34;scripts/require.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;My Sample Project\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; The attribute data-main defines your main entry point, the src attribute will load the require.js modular loader first, then run main.js\nrequirejs([\u0026#34;utils/someutils.js\u0026#34;], function(util) { //This function is called when scripts/helper/util.js is loaded. //If util.js calls define(), then this function is not fired until //util\u0026#39;s dependencies have loaded, and the util argument will hold //the module value for \u0026#34;helper/util\u0026#34;. }); Inside main.js you could load all dependencies in the first argument of function requirejs and this argument will be passed as the first argument of the callback function. Simply to say util is the alias of \u0026ldquo;utils/someutils.js\u0026rdquo;\nRequireJs also provide config functionality. Suppose you want to use jQuery. Inside your main.js you could add following code\nrequirejs.config({ baseUrl: \u0026#39;utils/lib\u0026#39;, paths: { // the left side is the module ID, // the right side is the path to // the jQuery file, relative to baseUrl. // Also, the path should NOT include // the \u0026#39;.js\u0026#39; file extension. This example // is using jQuery 1.9.0 located at // utils/lib/jquery-1.9.0.js, relative to // the HTML page. jquery: \u0026#39;jquery-1.9.0\u0026#39; } }); Here the baseUrl can make things much easier. Every time you want to use jQuery as a dependency, you don\u0026rsquo;t have to manually type the full path. Here is the example, suppose you want to create you own modular called addTitle.js\ndefine([\u0026#34;jquery\u0026#34;], function($){ // create your own module here // can use $ sign as we normally use jQuery }); CommonJs CommonJs is used in backend nodeJs. It\u0026rsquo;s the default module loader for nodeJs. It has two main systems: require/import and export.\nconst myModule = require(\u0026#39;path/to/mymodule\u0026#39;); // if this is a class, it\u0026#39;s really to be used const myObject = new myModule(); Some languages like Python and Java, you don\u0026rsquo;t explicitly to export your class or methods. However, in commonJs your have to explicitly to export your class and methods to be used\n// one way to export module.exports = class Dog{ constructor(name){ this.name = name; } bark(){ console.log(\u0026#34;Woooo\u0026#34;); } }; // or your can create class first, then export it class Dog{ // define your class first } module.exports = Dog; // only want to export certain methods module.exports.add = (a,b)=\u0026gt; a + b; I think the grammar of commonJs is more natural and it\u0026rsquo;s easier to understand\nES6/ES2015 ES6 is the new standard for javascript. Unfortunately, it\u0026rsquo;s not supported by modern browsers now. It has added many new features and syntactic sugar. These new features make developers who come from different language background feel easy to learn javascript. In order to use it, you have to use Babel to transpile it to current browsers supported javascript code.\n// this is your partical.js module export default class Partical{ // here is your implementation } // this is your personalizedmath.js module export function square(x) { return x * x; } export function pow3(x){ return x**3 } // this is your main.js module // since this is not default export, brakets is required import { square, pow3 } from \u0026#39;lib/personalizemath\u0026#39;; // since this is default export, there is no need to // add brakets, and you can rename to what name you want import Partical from \u0026#39;lib/partical\u0026#39; console.log(square(11)); // -\u0026gt; 121 console.log(pow3(2)); // -\u0026gt; 8 let myPartical = new Partical(); ES6 adds many features, it worths to study. Here is a short list for ES6 features\nArrow function (a,b)=\u0026gt; a+b default parameter const myfunc = function(height=50, color='red'){...} spread operator const listA = [1,2,3]; const listB = ['A', 'B', ...listA] // listB -\u0026gt; 'A', 'B', 1, 2, 3 Reference https://www.youtube.com/watch?v=JDDn57_z5Og\u0026amp;t=4s\u0026amp;frags=pl%2Cwn https://requirejs.org/ https://nodejs.org/ https://babeljs.io/ https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import ","date":1532649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532649600,"objectID":"74a86d2512980f4c3a9a95cd62f542b9","permalink":"https://terry-pan-dev.github.io/post/js-modular-history/","publishdate":"2018-07-27T00:00:00Z","relpermalink":"/post/js-modular-history/","section":"post","summary":"Learn the javascript modular history with code examples","tags":["javascript","modular loader","commonjs"],"title":"Javascript modular history","type":"post"},{"authors":["Terry Pan","Robert Ford"],"categories":null,"content":" Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://terry-pan-dev.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Terry Pan","Robert Ford"],"categories":null,"content":" Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://terry-pan-dev.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]