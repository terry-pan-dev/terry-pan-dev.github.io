<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Terry Pan</title>
    <link>https://terry-pan-dev.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Terry Pan</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://terry-pan-dev.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>SVM</title>
      <link>https://terry-pan-dev.github.io/post/support-vector-machine/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://terry-pan-dev.github.io/post/support-vector-machine/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;SVM was probably the most powerful classic machine learning algorithm before
neural network in practice.&lt;/p&gt;
&lt;p&gt;Before really dive into SVM, there are several terminologies worth to introduce,
one is call &lt;strong&gt;hyperplane&lt;/strong&gt;, it defines as in a p-dimensional space, a hyperplane
is a flat affine subspace of dimension p-1 &lt;span class=&#34;citation&#34;&gt;&lt;span class=&#34;nodecoration&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;[&lt;a href=&#34;#ref-james_introduction_2013&#34; role=&#34;doc-biblioref&#34;&gt;1&lt;/a&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, to make
it more concrete, in 3-d, the hyperplane is a plane. Whereas, in 2-d, the hyperplane
is a line.&lt;/p&gt;
&lt;p&gt;Next concept is called &lt;strong&gt;margin&lt;/strong&gt;, suppose we have a svm plot like following
&lt;img src=&#34;https://www.saedsayad.com/images/SVM_2.png&#34; alt=&#34;svm&#34; /&gt;
as you can see from the image above, the hyperplane is the dash line in the middle,
the &lt;strong&gt;margin&lt;/strong&gt; is the width from the solid line to dash line, we have &lt;strong&gt;support
vectors&lt;/strong&gt; to define margin. Which are the closest point to the hyperplane. However,
we may not always have such perfect case. Suppose data point are interleaving,
we can not find a hyperplane perfectly separate two classes. However, if we
can tolerate certain level of misclassifying and find a hyperplane to
minimize this error, we call this &lt;strong&gt;soft margin&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximal-margin-classifier&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Maximal margin classifier&lt;/h3&gt;
&lt;p&gt;I have to give a math definition for later use, to define a hyperplane
in math equation
&lt;span class=&#34;math display&#34;&gt;\[
\beta_0 + \beta_1X_1 + \beta_2X2 + \cdots + \beta_pX_p = 0
\]&lt;/span&gt;
if &lt;span class=&#34;math inline&#34;&gt;\(p=2\)&lt;/span&gt; we have a line, if &lt;span class=&#34;math inline&#34;&gt;\(p=3\)&lt;/span&gt; we will have a plane. Supposing we have a point
in p-dimension that satisfies the equation above, we say that this point is sitting
on this plane. If not, this point is either in
&lt;span class=&#34;math display&#34;&gt;\[
\beta_0 + \beta_1X_1 + \beta_2X2 + \cdots + \beta_pX_p &amp;gt; 0
\]&lt;/span&gt;
or
&lt;span class=&#34;math display&#34;&gt;\[
\beta_0 + \beta_1X_1 + \beta_2X2 + \cdots + \beta_pX_p &amp;lt; 0
\]&lt;/span&gt;
we know that the middle dash line is our decision boundary. Therefore, any point
lies on the line equal to zero. Otherwise, not greater than or less than zero.
Moreover, the larger the value the longer the distance between the point and
the line. Therefore, if you only need decide the margin, we do not need all
points, we know need three as above which we call it &lt;strong&gt;support vectors&lt;/strong&gt;. This
seems like a greate property, since we do not have to calculate many observations.&lt;/p&gt;
&lt;p&gt;for people who have used sklearn might notice, the svm classifier in sklearn
has a same parameter called &lt;code&gt;C&lt;/code&gt;. However, they are opposite each other, the &lt;code&gt;C&lt;/code&gt;
in sklearn is the penalty term. Which means if you give a big &lt;code&gt;C&lt;/code&gt; value, when
the classifier misclassifies points, it will get big penalty, in other words,
the classifier must make less misclassification to reduce the error or you could
say narrowing down the margin and vise versa.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-linear-decision-boundary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-linear decision boundary&lt;/h3&gt;
&lt;p&gt;suppose we have dataset like following:
&lt;img src=&#34;https://miro.medium.com/max/974/1*1o_4zV7Js_65TThqlDeGsg.png&#34; alt=&#34;non-linear&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s obviousely that we cannot use linear classifier to make the decision boundary
any more. However, what if we can transform the original feature space to higher
order? we have an similar example like the right side of above image.
&lt;img src=&#34;https://d3i71xaburhd42.cloudfront.net/7cb5f30af11f0ed5f88856a10da65d70b0a04d98/2-Figure1-1.png&#34; alt=&#34;kernel&#34; /&gt;
It has become linearly separable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body&#34;&gt;
&lt;div id=&#34;ref-james_introduction_2013&#34; class=&#34;csl-entry&#34;&gt;
&lt;div class=&#34;csl-left-margin&#34;&gt;1. &lt;/div&gt;&lt;div class=&#34;csl-right-inline&#34;&gt;&lt;div class=&#34;csl-left-margin&#34;&gt;&lt;strong&gt;An &lt;span&gt;Introduction&lt;/span&gt; to &lt;span&gt;Statistical&lt;/span&gt; &lt;span&gt;Learning&lt;/span&gt;&lt;/strong&gt;&lt;/div&gt; [Internet].&lt;div class=&#34;csl-block&#34;&gt;James G&lt;/div&gt;, &lt;div class=&#34;csl-block&#34;&gt;Witten D&lt;/div&gt;, &lt;div class=&#34;csl-block&#34;&gt;Hastie T&lt;/div&gt;, &lt;div class=&#34;csl-block&#34;&gt;Tibshirani R&lt;/div&gt;. New York, NY: Springer New York; 2013 [cited 2019 Nov 8]. Available from: &lt;a href=&#34;http://link.springer.com/10.1007/978-1-4614-7138-7&#34;&gt;http://link.springer.com/10.1007/978-1-4614-7138-7&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Decision tree</title>
      <link>https://terry-pan-dev.github.io/post/decision-tree/</link>
      <pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://terry-pan-dev.github.io/post/decision-tree/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Although nowadays deep learning is gaining more attention than classic machine
learning methods, classic machine learning algorithm is still have its advantage
than deep learning cannot beat, like simplicity and interpretability. Among
classic machine learning algorithm, decision tree or more accurate its variation
like random forest or bagging is widely used in industry. The beauty of decision
tree is that it closes to human making decision.&lt;/p&gt;
&lt;p&gt;There are several terms worth to explain in front. Here is an image to help me
to illustrate it
&lt;img src=&#34;https://discourse-cloud-file-uploads.s3.dualstack.us-west-2.amazonaws.com/business6/uploads/analyticsvidhya/original/2X/7/72d79a7b7841d04a077c5ac7a01590c341d9a041.png&#34; alt=&#34;dt&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Normally, a tabular data is &lt;strong&gt;binary splitting&lt;/strong&gt; into the tree like a bove, we are
choosing a feature (colume) to split the data (row), the decision node we call
it &lt;strong&gt;internal node&lt;/strong&gt;, then we recursively split the tree until we reach certain
threshold for example only two observations in the &lt;strong&gt;terminal node&lt;/strong&gt;, terminal
node is the end of a tree, we call it leaf node as well.&lt;/p&gt;
&lt;p&gt;As you can see above, the first internal node probably the most important factor
among other nodes, because based on this decision we decide where to go. Therefore,
this nature makes decision easy to interpret. Moreover, each terminal node is
actually divided into rectangle as you can see from the left on the image above.&lt;/p&gt;
&lt;p&gt;So is decision tree able to be used in regression and classification tasks as other
machine learning algorithms?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-tree&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression tree&lt;/h3&gt;
&lt;p&gt;Regression tree can be used in a regression task like linear regression, so how
does regression do a prediction? A regression tree predicted value is the mean
response of the training observations &lt;span class=&#34;citation&#34;&gt;&lt;span class=&#34;csl-baseline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;[&lt;a href=&#34;#ref-james_introduction_2013&#34; role=&#34;doc-biblioref&#34;&gt;1&lt;/a&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. To make it
simple, when we built our decision tree, the predicted value is just the average
value in each terminal nodes or each rectangle. The way to decide the splitting
value is by minimize the mean value with each observations in each rectangle.
&lt;span class=&#34;math display&#34;&gt;\[
\sum_{j=1}^J\sum_{i\in R_j} (y_i-\hat{y}_{R_j})^2
\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(R_j\)&lt;/span&gt; is the rectangle or terminal node. What we have to do is just minimize
the total error. When we doing a prediction, we just give a training observation
based on decision we find the rectangle it belongs to, then we find the mean
value inside that region. So how does a classification task can be done by decision
tree?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-tree&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Classification tree&lt;/h3&gt;
&lt;p&gt;Classification tree is nothing special than a regression tree, only difference
is the error measurement. For classfication problems, we don’t have quantitative
ground truth, instead we have qualitative value. Therefore, one measure called
&lt;strong&gt;Gini index&lt;/strong&gt; was introduced, Gini index is defined by
&lt;span class=&#34;math display&#34;&gt;\[
G = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})
\]&lt;/span&gt;
this is a measure of total variance across K classes, Where &lt;span class=&#34;math inline&#34;&gt;\(\hat{p}_{mk}\)&lt;/span&gt; is
the proportion of observations in the mth region (rectangle) for kth class. Gini
index measures the &lt;strong&gt;purity&lt;/strong&gt; of nodes, or call it &lt;strong&gt;impurity&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;&lt;span class=&#34;csl-baseline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;[&lt;a href=&#34;#ref-alpaydin_introduction_2010&#34; role=&#34;doc-biblioref&#34;&gt;2&lt;/a&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
why? as you can see the smaller the value the more a node from single class.
in other words, this node is not impure. Which more close to human intuition.
Another measure is called &lt;strong&gt;entropy&lt;/strong&gt;, it is defined as
&lt;span class=&#34;math display&#34;&gt;\[
E = -\sum_{k=1}^K \hat{p}_{mk}log_2(\hat{p}_{mk})
\]&lt;/span&gt;
as you can see, this measurement is close to Gini index measurement. Here I created
an artificial dataset to demostrate how to use entropy. Supposing we have 2-class
dataset like following&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;2.3&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1.8&lt;/td&gt;
&lt;td&gt;3.2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1.3&lt;/td&gt;
&lt;td&gt;3.3&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2.1&lt;/td&gt;
&lt;td&gt;3.1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;2.3&lt;/td&gt;
&lt;td&gt;3.4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2.8&lt;/td&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;if we split the tree based on &lt;span class=&#34;math inline&#34;&gt;\(x1 &amp;lt;= 1.8\)&lt;/span&gt;, the node on the left will purely
belongs to class 0, where the right node will all belongs to class 1. This is
a better choice against &lt;span class=&#34;math inline&#34;&gt;\(x2\)&lt;/span&gt;. Why? because the entropy
for the left node is 0, and so as right node. Whereas, if we choose &lt;span class=&#34;math inline&#34;&gt;\(x2 &amp;lt;= 3\)&lt;/span&gt; as
our splitting node, we will have entropy for left node is 0, but for right node
we have entropy &lt;span class=&#34;math inline&#34;&gt;\((-0.4*log(0.6)) + (-0.6*log(0.6)) = 0.97\)&lt;/span&gt;. Which compared to
splitting by &lt;span class=&#34;math inline&#34;&gt;\(x1\)&lt;/span&gt; is less optimal.&lt;/p&gt;
&lt;p&gt;Here is a plot to show the differences for different measurement methods &lt;span class=&#34;citation&#34;&gt;&lt;span class=&#34;csl-baseline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;[&lt;a href=&#34;#ref-podgorelec2002decision&#34; role=&#34;doc-biblioref&#34;&gt;3&lt;/a&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://sebastianraschka.com/images/faq/decision-tree-binary/overview-plot.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;measure&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pruning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pruning&lt;/h3&gt;
&lt;p&gt;when you don’t specify the max depth for decision tree, decision tree will lead
to overfitting. Pruning is for prevent overfitting, there are two strategies
to prun a tree, one is called prepruning, which stop earlier when training a
tree. Whereas, another is called postpruning, which allows the tree grow freely
then pruning the tree. Each has its one advantage over other, for example prepruning
will training fast, whereas, postpruning usually has a good accuracy over prepruning.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pros-cons&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pros, Cons&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;trees are easy to explain to people (+)&lt;/li&gt;
&lt;li&gt;trees can be displayed graphically (+)&lt;/li&gt;
&lt;li&gt;trees can easily handle qualitative variable without creating dummy variable (+)&lt;/li&gt;
&lt;li&gt;no need to center or scale data (+)&lt;/li&gt;
&lt;li&gt;no as accurate as other regression and classification methods (-)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, by adding more decision trees, the power of decision trees emerge,
one representative algorithm is random forest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;random foreset is an ensemble learning algorithm, as the name shows, it consists
many decision trees, it also utilizes a statistical method called bootstrap,
bootstrap is a way to sample data from a limited datasets, so that we can have
enough dataset to train. random forest is similar like bagging algorithm
the difference between bagging algorithm is that random forest using fewer
predictor variable (feature) to training. Which will help reduce the correlation
between predictor variables. One advantage of random forest is that even add
more decision trees, there will not lead to overfitting. There are other algorithms
based on decision tree, like Adaboosting, XGBoosting, CART, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tips&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tips&lt;/h3&gt;
&lt;p&gt;Here are some tips from practice &lt;span class=&#34;citation&#34;&gt;&lt;span class=&#34;csl-baseline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;[&lt;a href=&#34;#ref-boettcher2016online&#34; role=&#34;doc-biblioref&#34;&gt;4&lt;/a&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; regard to &lt;code&gt;sklearn&lt;/code&gt;.
- consider performing dimension reduction before training model
- using &lt;code&gt;depth=3&lt;/code&gt; as an starting tree to get a feel how the tree look like
- use &lt;code&gt;max_depth&lt;/code&gt; to control size of the tree to avoid overfitting
- when using random forest, choose the training feature as &lt;code&gt;sqrt(total feature)&lt;/code&gt;
- when using boosting, a typical learning rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda=(0.01,0.001)\)&lt;/span&gt; will be a good start&lt;/p&gt;
&lt;p&gt;demo for &lt;a href=&#34;https://github.com/shuishoudage/ML_Note/blob/master/classic_ml/decision_tree_in_r.ipynb&#34;&gt;decision tree&lt;/a&gt;
### References&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-james_introduction_2013&#34;&gt;
&lt;p&gt;1. &lt;strong&gt;An Introduction to Statistical Learning&lt;/strong&gt; [Internet].&lt;br /&gt;
James G, Witten D, Hastie T, Tibshirani R. &lt;br /&gt;
New York, NY: Springer New York; 2013 [cited 2019 Nov 8]. Available from: &lt;a href=&#34;http://link.springer.com/10.1007/978-1-4614-7138-7&#34;&gt;http://link.springer.com/10.1007/978-1-4614-7138-7&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-alpaydin_introduction_2010&#34;&gt;
&lt;p&gt;2. &lt;strong&gt;Introduction to machine learning&lt;/strong&gt;.&lt;br /&gt;
Alpaydin E. &lt;br /&gt;
2nd ed. Cambridge, Mass: MIT Press; 2010. &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-podgorelec2002decision&#34;&gt;
&lt;p&gt;3. &lt;strong&gt;Decision trees: An overview and their use in medicine&lt;/strong&gt;.&lt;br /&gt;
Podgorelec V, Kokol P, Stiglic B, Rozman I. &lt;br /&gt;
Journal of medical systems 2002;26(5):445–63. &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-boettcher2016online&#34;&gt;
&lt;p&gt;4. &lt;strong&gt;The online teaching survival guide: Simple and practical pedagogical tips&lt;/strong&gt;.&lt;br /&gt;
Boettcher JV, Conrad R-M. &lt;br /&gt;
John Wiley &amp;amp; Sons; 2016. &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>LSTM</title>
      <link>https://terry-pan-dev.github.io/post/lstm/</link>
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://terry-pan-dev.github.io/post/lstm/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;LSTM stards for Long-short term memory, it is a variant of RNN network. The strong
part of LSTM archetect is it has memory build-in. Imaging the daily conversation,
we are able to predict next words based on the context of the conversation because
we have remembered the important of part of the context. See example below,&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;images/ad_pre.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;ad_pre&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What you can remember after several days probabily only a few key information,
and you choose to forget irrelavant information. Here is the things you possible
remember after several days&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;images/ad_post.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;ad_post&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As LSTM can remember through time. Therefore, it has wide range of applications
like text/speech translation, speech to text translation, audio/video prediction&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Limitations&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Maximum likelihood estimation</title>
      <link>https://terry-pan-dev.github.io/post/maximum-likelihood-estimation/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://terry-pan-dev.github.io/post/maximum-likelihood-estimation/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Maximum likelihood is a fairly common optimization tool in machine learning&lt;/p&gt;
&lt;p&gt;There are many books discussed about MLE. However, the derivation of formulas
are often not clear. Thus in this article I am going to disclose of several
MLE formulas step by step, hoping that will benefit all machine learning
practitioners.&lt;/p&gt;
&lt;p&gt;to be continue…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://terry-pan-dev.github.io/post/linear-regression/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://terry-pan-dev.github.io/post/linear-regression/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Linear regression is probably the first topic that every machine learning
course will talk. It is simple and often prodivde and adequate and interpretable
description of how the inputs affect the output &lt;span class=&#34;citation&#34;&gt;&lt;span class=&#34;csl-baseline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;[&lt;a href=&#34;#ref-friedman2001elements&#34; role=&#34;doc-biblioref&#34;&gt;1&lt;/a&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.
I will try my best to explain linear regression in a simple way but also with
depth insight. For those who scare of math, it is okey just give a glance.&lt;/p&gt;
&lt;p&gt;Let’s start from a very simple equation from high school.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y = ax + b
\]&lt;/span&gt;
Here y is outcome or sometimes we will written as &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, it is the same thing
but written in a different format. a is the slope and b is the intercept. Let’s
giving a plot.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://terry-pan-dev.github.io/post/linear-regression/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
This example is set intercept to 1.4 and slope as 0.5. For this example, we only
have one input (x) and one output (y)&lt;/p&gt;
&lt;p&gt;In real world application, we normally have more than one feature. Suppose
we have an input vector &lt;span class=&#34;math inline&#34;&gt;\(X^T=(X_1, X_2, \dots, X_p)\)&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-3&#34;&gt;Table 1: &lt;/span&gt;A table for advertising dataset
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
X
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
TV
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Radio
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Newspaper
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Sales
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
230.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
151.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
41.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
180.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here the input features we could think as &lt;strong&gt;X1=TV&lt;/strong&gt;, &lt;strong&gt;X2=Radio&lt;/strong&gt;
and &lt;strong&gt;X3=Newspaper&lt;/strong&gt;. and the y is &lt;strong&gt;Sales&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have a simple model for multiple features&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(X)=\beta_{0}+\sum_{j=1}^{p} X_{j} \beta_{j}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s plot a regression model for iris dataset to see what exactly going on.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df, aes(TV, Sales)) +
  geom_point() +
  geom_smooth(method = &amp;#39;lm&amp;#39;, formula = y~x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://terry-pan-dev.github.io/post/linear-regression/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What we like to do is reduce the residuals.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;residuals.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;png&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So finally, what we want to do is just minimize the residual sum of squares, which
is the equation following:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} \operatorname{RSS}(\beta) &amp;amp;=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{2} \\ &amp;amp;=\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2} \end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To make the formula clear little bit. We could using vectorization or vector
notation to represent it.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\operatorname{RSS}(\beta)=(\mathbf{y}-\mathbf{X} \beta)^{T}(\mathbf{y}-\mathbf{X} \beta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;let’s take the derivative respect to &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.
&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial \mathrm{RSS}}{\partial \beta}=-2 \mathbf{X}^{T}(\mathbf{y}-\mathbf{X} \beta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because, if we check the RSS equation above it is a quadratic function. Therefore,
this equation is always having a global minimal value. Let’s just set the derivative
equal zero, then find what the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;-2 \mathbf{X}^{T}(\mathbf{y}-\mathbf{X} \beta)=0 \\ &amp;amp;\mathbf{X}^{T}(\mathbf{y}-\mathbf{X} \beta)=0 \quad (1)\\
&amp;amp;\mathbf{X}^{T}\mathbf{y}-\mathbf{X}^{T}\mathbf{X} \beta=0 \quad (2) \\
&amp;amp;\mathbf{X}^{T}\mathbf{y}=\mathbf{X}^{T}\mathbf{X} \beta \quad(3)\\
&amp;amp;(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}=(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{X} \beta \quad (4)\\
&amp;amp;(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y} = \beta \quad (5)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s break it down, the first step we just divide both side by -2, hence we can
eliminate the constant. Second step we just apply distribution rule, the third
one is to make the equation equal, we just move &lt;span class=&#34;math inline&#34;&gt;\(-X^tX\beta\)&lt;/span&gt; to the right side.
Step 4 is just using a little trick to make &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; to an identity matrix by
multiply &lt;span class=&#34;math inline&#34;&gt;\((X^TX)^{-1}\)&lt;/span&gt;, finally, we get &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is obviously that if we multiply &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; to the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; we can calculate the estimated
&lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt; value&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Case study&lt;/h3&gt;
&lt;p&gt;It is important for an analyst to understand the output of a linear model. It is
not just blindly using some packages without thinking. Here I take an example
from book An Introduction to Statistical Learning &lt;span class=&#34;citation&#34;&gt;&lt;span class=&#34;csl-baseline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;[&lt;a href=&#34;#ref-james2013introduction&#34; role=&#34;doc-biblioref&#34;&gt;2&lt;/a&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s start from single variable linear regression model, then try to give
a short explanation to the output. Here we use Boosten dataset from R package
of MASS, this dataset records 506 neighborhoods around Boosten, there are several
feature columns, like &lt;code&gt;crim&lt;/code&gt; indicates per capita crime rate by town, &lt;code&gt;rm&lt;/code&gt; means
the average number of rooms per dwelling and &lt;code&gt;ptratia&lt;/code&gt; means pupil-teacher ratio
by town.&lt;/p&gt;
&lt;p&gt;Here we use feature &lt;code&gt;lstat&lt;/code&gt; (lower status of the population) to predict
&lt;code&gt;medv&lt;/code&gt;(median value of owner-occupied homes in $1000).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.fit &amp;lt;- lm(medv ~ lstat, data = MASS::Boston)
summary(lm.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = medv ~ lstat, data = MASS::Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.168  -3.990  -1.318   2.034  24.500 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 34.55384    0.56263   61.41   &amp;lt;2e-16 ***
## lstat       -0.95005    0.03873  -24.53   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 6.216 on 504 degrees of freedom
## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 
## F-statistic: 601.6 on 1 and 504 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s interpret the summary one by one. The first is the call field, it is just
simply the formula of this model, nothing special. Next one is the statistical
summary for residuals, actually we can use box plot to show it more concretely.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data.frame(resi = residuals(lm.fit)), aes(x = &amp;quot;&amp;quot;, y = resi )) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://terry-pan-dev.github.io/post/linear-regression/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the plot above matches the summary. The middle thicker horizontal
line is the median (-1.318) in the summary, and the line above and below the
median line is the first quantile (1Q) and third quantile (3Q). The points on
the top and bottom is the min and max value.&lt;/p&gt;
&lt;p&gt;The next important field is Coefficients, by observing this field, we can
find the how predict variables affect the response variable, this is why linear
regression model is an interpretable model.&lt;/p&gt;
&lt;p&gt;If you recall from the beginning of this blog, there is a formula &lt;span class=&#34;math inline&#34;&gt;\(y = ax + b\)&lt;/span&gt;
here the intercept is 34.55384, we can think this value is as &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; in the formula
and &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is &lt;code&gt;lstat&lt;/code&gt; which is -0.95005. Just by these information, we could know
that &lt;code&gt;lstat&lt;/code&gt; and &lt;code&gt;medv&lt;/code&gt; is negatively related, which means when &lt;code&gt;lstat&lt;/code&gt; increase
1 unit, &lt;code&gt;medv&lt;/code&gt; will decrease -0.95005. &lt;strong&gt;Std. Error&lt;/strong&gt; here can be use to calculate
the confidence interval and &lt;strong&gt;t value&lt;/strong&gt; is used to calculate p-value. Therefore,
if we really want to know the confidence interval for coefficients, we can use
function confint in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(lm.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 2.5 %     97.5 %
## (Intercept) 33.448457 35.6592247
## lstat       -1.026148 -0.8739505&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The confidence interval just like asking people to guess something, suppose ask
a veteran mechanics what is the probability that a certain engine will be broken,
he will give his experienced guess but with amount of uncertainty, but if you ask
an amateur, he will also give his guess but with a more large range of uncertainty.&lt;/p&gt;
&lt;p&gt;Just by knowing the confidence interval you could know that if a coefficient
significant of not. When there is a 0 appears between the confidence interval
you could confirm that this coefficient is not significant, because there may
have a chance the coefficient becomes 0. Which makes that input feature no
meaning at all.&lt;/p&gt;
&lt;p&gt;After building the model, we can use this model to predict new coming data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.pred &amp;lt;- predict(lm.fit, data.frame(lstat=(c(5,10,15))), interval = &amp;quot;confidence&amp;quot;)
lm.pred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        fit      lwr      upr
## 1 29.80359 29.00741 30.59978
## 2 25.05335 24.47413 25.63256
## 3 20.30310 19.73159 20.87461&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the test data is confirming our interpretation above, as &lt;code&gt;lstat&lt;/code&gt; increasing
&lt;code&gt;medv&lt;/code&gt; decreasing. Notice, here we use a special parameter, &lt;em&gt;interval=“confidence”&lt;/em&gt;.
this parameter will give the prediction around the mean of the prediction. To interpret the result above, with 10 &lt;code&gt;lstat&lt;/code&gt; on average the value for &lt;code&gt;medv&lt;/code&gt; is
between 24.47 and 25.63.&lt;/p&gt;
&lt;p&gt;We can set the parameter inverval as “prediction” as well. However, this prediction
only give the prediction interval around a single value. This setting is highly
rely on residuals are normally distributed. I will give the methods how to
test the normality of residuals in the &lt;a href=&#34;#Caveats&#34;&gt;caveat&lt;/a&gt; section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Caveats&lt;/h3&gt;
&lt;p&gt;Linear regression model has its limitation or more precisely saying it has assumption
For example, as the name indicates, there must be a linear relationship between
predict variables and response variable, because the solution will either line
plane or hyperplane which is not curvy. I list several assumptions regard to
linear regression.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;residuals are roughly normally distributed&lt;/li&gt;
&lt;li&gt;residuals are independent&lt;/li&gt;
&lt;li&gt;linear relation between features and output&lt;/li&gt;
&lt;li&gt;no or little multicollinearity&lt;/li&gt;
&lt;li&gt;homoscedasticity (residuals agains fitted value should keep constant)&lt;/li&gt;
&lt;li&gt;sensitive to outliers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are several ways to test the assumptions&lt;/p&gt;
&lt;div id=&#34;residual-normality&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;residual normality&lt;/h4&gt;
&lt;p&gt;By checking residuals normality, we can use Q-Q plot,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.fit &amp;lt;- lm(medv ~ rm + lstat + crim + dis, data = Boston)
ggqqplot(residuals(lm.fit))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://terry-pan-dev.github.io/post/linear-regression/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How to interpret this plot? Suppose the residuals are following a normal distribution,
the points above should be roughly located around the slope line and should be
with inside the 95% confidence curvy dashed line.&lt;/p&gt;
&lt;p&gt;In a algorithmic way to check, we could utilize Shapiro-Wilk test &lt;span class=&#34;citation&#34;&gt;&lt;span class=&#34;csl-baseline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;[&lt;a href=&#34;#ref-Lohninger2012Oct&#34; role=&#34;doc-biblioref&#34;&gt;3&lt;/a&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shapiro.test(residuals(lm.fit))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(lm.fit)
## W = 0.91081, p-value &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the result p-value indicates (&amp;lt;0.05). We can reject the null hypothesis
(normally distributed). Therefore, the residuals is not normally distributed.
Here we can see, it is important to do both visual and algorithmic test, the plot
above shows us a roughly normal distributed data. However, the test indicates
this is &lt;strong&gt;not&lt;/strong&gt; a normally distributed data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;residuals-independency&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;residuals independency&lt;/h4&gt;
&lt;p&gt;By plotting the residuals along with the order of &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; we can measure the residuals
independency.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(sequence(nrow(Boston)), residuals(lm.fit))
abline(h=0, col=&amp;#39;red&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://terry-pan-dev.github.io/post/linear-regression/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Independent residuals should be located around 0 and randomly scattered.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linearity&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;linearity&lt;/h4&gt;
&lt;p&gt;linearity is just checking if the predict variables (input X) has the linear
relation between the response variable. After all, we use linear regression to
create model, if there is no linear relation we have to use other non-linear
model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crPlots(lm(medv ~ rm + lstat + crim + dis, data = Boston))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://terry-pan-dev.github.io/post/linear-regression/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;what we most case is if there is any pattern inside the plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(gvlma::gvlma(x = lm.fit))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = medv ~ rm + lstat + crim + dis, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -19.006  -3.099  -1.047   1.885  26.571 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  2.23065    3.32214   0.671    0.502    
## rm           4.97649    0.43885  11.340  &amp;lt; 2e-16 ***
## lstat       -0.66174    0.05101 -12.974  &amp;lt; 2e-16 ***
## crim        -0.12810    0.03209  -3.992 7.53e-05 ***
## dis         -0.56321    0.13542  -4.159 3.76e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 5.403 on 501 degrees of freedom
## Multiple R-squared:  0.6577, Adjusted R-squared:  0.6549 
## F-statistic: 240.6 on 4 and 501 DF,  p-value: &amp;lt; 2.2e-16
## 
## 
## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
## Level of Significance =  0.05 
## 
## Call:
##  gvlma::gvlma(x = lm.fit) 
## 
##                     Value   p-value                   Decision
## Global Stat        638.73 0.000e+00 Assumptions NOT satisfied!
## Skewness           143.63 0.000e+00 Assumptions NOT satisfied!
## Kurtosis           289.67 0.000e+00 Assumptions NOT satisfied!
## Link Function      175.67 0.000e+00 Assumptions NOT satisfied!
## Heteroscedasticity  29.76 4.902e-08 Assumptions NOT satisfied!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;hemoscedasticity&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;hemoscedasticity&lt;/h4&gt;
&lt;p&gt;For checking hemoscedasticity. Which means the variance of residuals should
be constant, see the plot below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(2,2))
plot(lm.fit, pch = 16, cex = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://terry-pan-dev.github.io/post/linear-regression/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;
We are intereted in the left side two plots. To express homoscedasticity in a plot,
it would look like a roughly flat line with random points around it. However, in
our case here &lt;span class=&#34;citation&#34;&gt;&lt;span class=&#34;csl-baseline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;[&lt;a href=&#34;#ref-BibEntry2016Jan&#34; role=&#34;doc-biblioref&#34;&gt;4&lt;/a&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. We could find the red line is curvy and the
points have a growth trend from left to right.&lt;/p&gt;
&lt;p&gt;If you think visualization is not enough to determine if a model is hemoscedasticity,
There are several algorithmic ways to test as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;car::ncvTest(lm.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 0.4130551, Df = 1, p = 0.52042&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By checking the p-value, we could see that is significant (&amp;lt;0.05) enough to reject
the null hypothesis (variance is constant).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outliers&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;outliers&lt;/h4&gt;
&lt;p&gt;Because we use RSS to estimate the accuracy, it is obviousely that outliers
will highly influence the RSS. Which makes the wrong interpretation to the model.&lt;/p&gt;
&lt;p&gt;A data point has high leverage, if it has extreme predictor x values &lt;span class=&#34;citation&#34;&gt;&lt;span class=&#34;csl-baseline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;[&lt;a href=&#34;#ref-BibEntry2019Jun&#34; role=&#34;doc-biblioref&#34;&gt;5&lt;/a&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Let’s using &lt;em&gt;Residuals vs Leverage&lt;/em&gt; plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,2))
plot(lm.fit, 4)
plot(lm.fit, 5, pch = 16, cex = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://terry-pan-dev.github.io/post/linear-regression/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot above shows that #23, #49 and #39 are influenced points. Among those
three points, #23 and #49 are close to 3 standard deviation. Which is more influential
than #39. However, are these outliers really influence the result of the regression
analysis? Here we need one metric called Cook’s distance to check if data points
really influential.&lt;/p&gt;
&lt;p&gt;A rule of thumb is that an observation Cook’s distance exceeds
&lt;span class=&#34;math inline&#34;&gt;\(4/(n-p-1)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of observations and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the predictor
variables. Thus, we can calculate Cook’s distance for this model, which is
4/(50-1-1) = 0.08. We can add a horizontal line to the Cook’s distance plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;olsrr::ols_plot_cooksd_bar(lm.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://terry-pan-dev.github.io/post/linear-regression/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see above, data point #23 and #49 are really influence the result of
linear regression analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-friedman2001elements&#34;&gt;
&lt;p&gt;1. &lt;strong&gt;The elements of statistical learning&lt;/strong&gt;.&lt;br /&gt;
Friedman J, Hastie T, Tibshirani R. &lt;br /&gt;
Springer series in statistics New York; 2001. &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-james2013introduction&#34;&gt;
&lt;p&gt;2. &lt;strong&gt;An introduction to statistical learning&lt;/strong&gt;.&lt;br /&gt;
James G, Witten D, Hastie T, Tibshirani R. &lt;br /&gt;
Springer; 2013. &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Lohninger2012Oct&#34;&gt;
&lt;p&gt;3. &lt;strong&gt;Shapiro-Wilk Test&lt;/strong&gt; [Internet].&lt;br /&gt;
Lohninger H. &lt;br /&gt;
2012;Available from: &lt;a href=&#34;http://www.statistics4u.info/fundstat_eng/ee_shapiro_wilk_test.html&#34;&gt;http://www.statistics4u.info/fundstat_eng/ee_shapiro_wilk_test.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-BibEntry2016Jan&#34;&gt;
&lt;p&gt;4. &lt;strong&gt;How to detect heteroscedasticity and rectify it?&lt;/strong&gt; [Internet].R-bloggers2016;Available from: &lt;a href=&#34;https://www.r-bloggers.com/how-to-detect-heteroscedasticity-and-rectify-it&#34;&gt;https://www.r-bloggers.com/how-to-detect-heteroscedasticity-and-rectify-it&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-BibEntry2019Jun&#34;&gt;
&lt;p&gt;5. &lt;strong&gt;Linear Regression Assumptions and Diagnostics in R: Essentials - Articles - STHDA&lt;/strong&gt; [Internet].2019;Available from: &lt;a href=&#34;http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials&#34;&gt;http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
